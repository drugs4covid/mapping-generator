{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting owlready2\n",
      "  Using cached Owlready2-0.36.tar.gz (23.8 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: owlready2\n",
      "  Building wheel for owlready2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for owlready2: filename=Owlready2-0.36-py3-none-any.whl size=19824048 sha256=41ae99c1c02afc0f487577bd909a968680101164e758bdafed4cf03ded0eb769\n",
      "  Stored in directory: /home/pablo/.cache/pip/wheels/4a/4e/30/fb2dd09a5e7f330431cfc21beb1260f42de44e8a20d2d7b087\n",
      "Successfully built owlready2\n",
      "Installing collected packages: owlready2\n",
      "Successfully installed owlready2-0.36\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install owlready2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mysql-connector-python in /home/pablo/.local/lib/python3.8/site-packages (8.0.28)\n",
      "Requirement already satisfied: protobuf>=3.0.0 in /usr/lib/python3/dist-packages (from mysql-connector-python) (3.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rdflib\n",
      "  Downloading rdflib-6.1.1-py3-none-any.whl (482 kB)\n",
      "\u001b[K     |████████████████████████████████| 482 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting isodate\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 259 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/pablo/anaconda3/lib/python3.9/site-packages (from rdflib) (58.0.4)\n",
      "Requirement already satisfied: pyparsing in /home/pablo/anaconda3/lib/python3.9/site-packages (from rdflib) (3.0.4)\n",
      "Requirement already satisfied: six in /home/pablo/anaconda3/lib/python3.9/site-packages (from isodate->rdflib) (1.16.0)\n",
      "Installing collected packages: isodate, rdflib\n",
      "Successfully installed isodate-0.6.1 rdflib-6.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rdflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ontospy[FULL]\n",
      "  Using cached ontospy-1.9.9.4-py2.py3-none-any.whl (7.7 MB)\n",
      "Collecting pyfiglet\n",
      "  Using cached pyfiglet-0.8.post1-py2.py3-none-any.whl (865 kB)\n",
      "Requirement already satisfied: colorama in /usr/lib/python3/dist-packages (from ontospy[FULL]) (0.4.3)\n",
      "Requirement already satisfied: pyparsing in /home/pablo/.local/lib/python3.8/site-packages (from ontospy[FULL]) (3.0.7)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from ontospy[FULL]) (2.22.0)\n",
      "Collecting SPARQLWrapper\n",
      "  Using cached SPARQLWrapper-1.8.5-py3-none-any.whl (26 kB)\n",
      "Collecting rdflib\n",
      "  Using cached rdflib-6.1.1-py3-none-any.whl (482 kB)\n",
      "Requirement already satisfied: click in /home/pablo/.local/lib/python3.8/site-packages (from ontospy[FULL]) (8.0.3)\n",
      "Collecting html5lib\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting Django>=1.10.3\n",
      "  Downloading Django-4.0.3-py3-none-any.whl (8.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting Pygments==2.1.3\n",
      "  Using cached Pygments-2.1.3-py2.py3-none-any.whl (755 kB)\n",
      "Collecting asgiref<4,>=3.4.1\n",
      "  Using cached asgiref-3.5.0-py3-none-any.whl (22 kB)\n",
      "Collecting backports.zoneinfo\n",
      "  Downloading backports.zoneinfo-0.2.1-cp38-cp38-manylinux1_x86_64.whl (74 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sqlparse>=0.2.2\n",
      "  Using cached sqlparse-0.4.2-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: six>=1.9 in /usr/lib/python3/dist-packages (from html5lib->ontospy[FULL]) (1.14.0)\n",
      "Requirement already satisfied: webencodings in /home/pablo/.local/lib/python3.8/site-packages (from html5lib->ontospy[FULL]) (0.5.1)\n",
      "Collecting isodate\n",
      "  Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from rdflib->ontospy[FULL]) (45.2.0)\n",
      "Installing collected packages: Pygments, pyfiglet, sqlparse, isodate, html5lib, backports.zoneinfo, asgiref, rdflib, Django, SPARQLWrapper, ontospy\n",
      "  Attempting uninstall: Pygments\n",
      "    Found existing installation: Pygments 2.11.2\n",
      "    Uninstalling Pygments-2.11.2:\n",
      "      Successfully uninstalled Pygments-2.11.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "nbconvert 6.4.2 requires pygments>=2.4.1, but you have pygments 2.1.3 which is incompatible.\n",
      "jupyterlab-pygments 0.1.2 requires pygments<3,>=2.4.1, but you have pygments 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Django-4.0.3 Pygments-2.1.3 SPARQLWrapper-1.8.5 asgiref-3.5.0 backports.zoneinfo-0.2.1 html5lib-1.1 isodate-0.6.1 ontospy-1.9.9.4 pyfiglet-0.8.post1 rdflib-6.1.1 sqlparse-0.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ontospy[FULL] -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import csv\n",
    "import mysql.connector\n",
    "import numpy as numpy\n",
    "import json\n",
    "import ontospy\n",
    "from sqlite3 import Error\n",
    "from owlready2 import *\n",
    "from math import floor, ceil\n",
    "from enum import Enum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection is established:database.db\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "table AGENCY already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/pablo/Documentos/Github/mapping-generator/src/Generator.ipynb Cell 7'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pablo/Documentos/Github/mapping-generator/src/Generator.ipynb#ch0000006?line=13'>14</a>\u001b[0m     con\u001b[39m.\u001b[39mcommit()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pablo/Documentos/Github/mapping-generator/src/Generator.ipynb#ch0000006?line=15'>16</a>\u001b[0m \u001b[39m#http://vocab.gtfs.org/gtfs.ttl\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/pablo/Documentos/Github/mapping-generator/src/Generator.ipynb#ch0000006?line=16'>17</a>\u001b[0m sql_table(con)\n",
      "\u001b[1;32m/home/pablo/Documentos/Github/mapping-generator/src/Generator.ipynb Cell 7'\u001b[0m in \u001b[0;36msql_table\u001b[0;34m(con)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pablo/Documentos/Github/mapping-generator/src/Generator.ipynb#ch0000006?line=10'>11</a>\u001b[0m sql_file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAgencia.sql\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pablo/Documentos/Github/mapping-generator/src/Generator.ipynb#ch0000006?line=11'>12</a>\u001b[0m sql_as_string \u001b[39m=\u001b[39m sql_file\u001b[39m.\u001b[39mread()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/pablo/Documentos/Github/mapping-generator/src/Generator.ipynb#ch0000006?line=12'>13</a>\u001b[0m cursorObj\u001b[39m.\u001b[39;49mexecutescript(sql_as_string)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pablo/Documentos/Github/mapping-generator/src/Generator.ipynb#ch0000006?line=13'>14</a>\u001b[0m con\u001b[39m.\u001b[39mcommit()\n",
      "\u001b[0;31mOperationalError\u001b[0m: table AGENCY already exists"
     ]
    }
   ],
   "source": [
    "dataname = 'database.db'\n",
    "memory = ':memory:'\n",
    "try:\n",
    "    con = sqlite3.connect(dataname)\n",
    "    print(\"Connection is established:\" + dataname)\n",
    "except Error:\n",
    "    print (Error)\n",
    "        \n",
    "def sql_table(con):\n",
    "    cursorObj = con.cursor()\n",
    "    sql_file = open(\"Agencia.sql\")\n",
    "    sql_as_string = sql_file.read()\n",
    "    cursorObj.executescript(sql_as_string)\n",
    "    con.commit()\n",
    "\n",
    "#http://vocab.gtfs.org/gtfs.ttl\n",
    "sql_table(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_database():\n",
    "    try:\n",
    "        connection = mysql.connector.connect(host='localhost',\n",
    "                                     user='root',\n",
    "                                     password='gtfs',\n",
    "                                     database='gtfs',\n",
    "                                     charset='utf8mb4')\n",
    "\n",
    "        if connection.is_connected():\n",
    "                db_Info = connection.get_server_info()\n",
    "                print(\"Connected to MySQL Server version \", db_Info)\n",
    "                cursor = connection.cursor()\n",
    "                cursor.execute(\"select database();\")\n",
    "                record = cursor.fetchone()\n",
    "                print(\"You're connected to database: \", record)\n",
    "            \n",
    "        return connection, cursor\n",
    "\n",
    "    except Error as e:\n",
    "        print(\"Error while connecting to MySQL\", e)\n",
    "    #finally:\n",
    "    #    if connection.is_connected():\n",
    "    #        cursor.close()\n",
    "    #        connection.close()\n",
    "    #        print(\"MySQL connection is closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tables(connection, cursor):\n",
    "    cursor.execute(\"show tables;\")\n",
    "    myresult = cursor.fetchall()\n",
    "    return myresult\n",
    "def get_table_properties(table, cursor):\n",
    "    sql_select_Query = \"select * from \" + table\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(sql_select_Query)\n",
    "    # get all records\n",
    "    records = cursor.fetchall()\n",
    "    return cursor.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cur = connection.cursor()\n",
    "#cur.execute(\"SELECT * FROM AGENCY\")\n",
    "#cur.fetchall()\n",
    "cur.execute('DROP DATABASE IF EXISTS prueba.db')\n",
    "df = pd.read_sql_query(\"SELECT * FROM AGENCY\",con)\n",
    "print(df.to_string())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('examples/datasets/sql/1/AGENCY.csv')\n",
    "print(df.to_string()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadOntology(onto):\n",
    "    my_world = World()\n",
    "    ontology = onto\n",
    "    onto = my_world.get_ontology(ontology).load()\n",
    "    return onto, my_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'onto' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6853/685041449.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_iri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'onto' is not defined"
     ]
    }
   ],
   "source": [
    "print(onto.base_iri)\n",
    "print(onto.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ontology_classes(my_world):\n",
    "    try:\n",
    "        l = list(my_world.sparql(\"\"\"\n",
    "                   SELECT  DISTINCT ?x \n",
    "                   { ?x a rdfs:Class . }\n",
    "            \"\"\"))\n",
    "    except:\n",
    "        l = list(my_world.sparql(\"\"\"\n",
    "                   SELECT  DISTINCT ?x \n",
    "                   { ?x a owl:Class . }\n",
    "            \"\"\"))\n",
    "    class_list_names = []\n",
    "    classes = []\n",
    "    for elem in l:\n",
    "        terms = str(elem).split('.')\n",
    "        class_list_names.append((re.sub(']', '',terms.pop())))\n",
    "        classes.append(elem[0])\n",
    "    return class_list_names, classes\n",
    "\n",
    "def get_class_properties(my_world, class_name, prefix, uri):\n",
    "    t = re.sub('\\.', ':', str(class_name))\n",
    "    l = list(my_world.sparql(\"\"\"\n",
    "               PREFIX \"\"\" + prefix + \"\"\": \"\"\" + uri \n",
    "               + \"\"\"SELECT  DISTINCT ?p \n",
    "               { ?x a \"\"\" + t + \"\"\";\n",
    "                    ?p ?o. }\n",
    "        \"\"\"))\n",
    "    return l\n",
    "\n",
    "def get_ontology_properties(my_world, prefix, uri):\n",
    "    graph = my_world.as_rdflib_graph()\n",
    "    try:\n",
    "        l = list(graph.query_owlready(\"\"\"\n",
    "                   PREFIX \"\"\" + prefix + \"\"\": \"\"\" + uri\n",
    "                   + \"\"\"SELECT  DISTINCT ?p  \n",
    "                   { ?x a rdfs:Class;\n",
    "                        ?p ?o. }\n",
    "            \"\"\"))\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadOntology(uri):\n",
    "    return ontospy.Ontospy(uri, verbose=True, hide_implicit_preds=True, hide_implicit_types=True)\n",
    "\n",
    "#List<Classes>\n",
    "def get_onto_classes(ontology):\n",
    "    classes = []\n",
    "    for x in ontology.all_classes:\n",
    "        #print(\"Nombre: \", x.qname, \"Dominio: \", x.domain_of, \"Range: \", x.range_of)\n",
    "        #print(\"Nombre: \", x.qname, \"triples: \", x.triples)\n",
    "        #if (x.domain_of):\n",
    "        #print(x.describe())\n",
    "        classes.append(x)\n",
    "    return classes\n",
    "\n",
    "#List<Properties>\n",
    "def get_class_properties(ontology, o_class):\n",
    "    properties = []\n",
    "    total_prop = []\n",
    "    for x in ontology.all_properties:\n",
    "        for y in x.ranges:\n",
    "            if y.qname == o_class.qname:\n",
    "                properties.append(x)\n",
    "        for y in x.domains:\n",
    "            if y.qname == o_class.qname:\n",
    "                properties.append(x)\n",
    "    total_prop.append([o_class, properties])\n",
    "    return properties\n",
    "\n",
    "#List<Class, List<Properties>>\n",
    "def get_onto_properties(ontology, classes):\n",
    "    properties = []\n",
    "    for x in classes:\n",
    "        properties_class = get_class_properties(ontology, x)\n",
    "        if len(properties_class) != 0:\n",
    "            properties.append(properties_class)\n",
    "    return properties\n",
    "\n",
    "\n",
    "def printTurtle(x):\n",
    "    x.printSerialize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL Server version  8.0.26\n",
      "You're connected to database:  ('gtfs',)\n",
      "Total number of columns in table: AGENCY ('agency_id', 'agency_name', 'agency_url', 'agency_timezone', 'agency_lang', 'agency_phone', 'agency_fare_url')\n",
      "Total number of columns in table: CALENDAR ('service_id', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'start_date', 'end_date')\n",
      "Total number of columns in table: CALENDAR_DATES ('service_id', 'date', 'exception_type')\n",
      "Total number of columns in table: FEED_INFO ('feed_publisher_name', 'feed_publisher_url', 'feed_lang', 'feed_start_date', 'feed_end_date', 'feed_version')\n",
      "Total number of columns in table: FREQUENCIES ('trip_id', 'start_time', 'end_time', 'headway_secs', 'exact_times')\n",
      "Total number of columns in table: ROUTES ('route_id', 'agency_id', 'route_short_name', 'route_long_name', 'route_desc', 'route_type', 'route_url', 'route_color', 'route_text_color')\n",
      "Total number of columns in table: SHAPES ('shape_id', 'shape_pt_lat', 'shape_pt_lon', 'shape_pt_sequence', 'shape_dist_traveled')\n",
      "Total number of columns in table: STOPS ('stop_id', 'stop_code', 'stop_name', 'stop_desc', 'stop_lat', 'stop_lon', 'zone_id', 'stop_url', 'location_type', 'parent_station', 'stop_timezone', 'wheelchair_boarding')\n",
      "Total number of columns in table: STOP_TIMES ('trip_id', 'arrival_time', 'departure_time', 'stop_id', 'stop_sequence', 'stop_headsign', 'pickup_type', 'drop_off_type', 'shape_dist_traveled')\n",
      "Total number of columns in table: TRIPS ('route_id', 'service_id', 'trip_id', 'trip_headsign', 'trip_short_name', 'direction_id', 'block_id', 'shape_id', 'wheelchair_accessible')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mReading: <http://vocab.gtfs.org/gtfs.ttl>\u001b[0m\n",
      ".. trying rdf serialization: <turtle>\u001b[0m\n",
      "\u001b[1m..... success!\u001b[0m\n",
      "\u001b[37m----------\n",
      "Loaded 870 triples.\n",
      "----------\u001b[0m\n",
      "\u001b[32mRDF sources loaded successfully: 1 of 1.\u001b[0m\n",
      "\u001b[37m..... 'http://vocab.gtfs.org/gtfs.ttl'\u001b[0m\n",
      "\u001b[37m----------\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mScanning entities...\u001b[0m\n",
      "\u001b[2m----------\u001b[0m\n",
      "\u001b[2mOntologies.........: 1\u001b[0m\n",
      "\u001b[2mClasses............: 29\u001b[0m\n",
      "\u001b[2mProperties.........: 52\u001b[0m\n",
      "\u001b[2m..annotation.......: 0\u001b[0m\n",
      "\u001b[2m..datatype.........: 0\u001b[0m\n",
      "\u001b[2m..object...........: 0\u001b[0m\n",
      "\u001b[2mConcepts (SKOS)....: 0\u001b[0m\n",
      "\u001b[2mShapes (SHACL).....: 0\u001b[0m\n",
      "\u001b[2m----------\u001b[0m\n",
      "\n",
      "Reading  Ontology...\n",
      "\n",
      "By library classes method:  [<Class *http://www.w3.org/ns/dcat#Dataset*>, <Class *http://xmlns.com/foaf/0.1/Agent*>, <Class *http://vocab.gtfs.org/terms#Agency*>, <Class *http://vocab.gtfs.org/terms#CalendarDateRule*>, <Class *http://vocab.gtfs.org/terms#CalendarRule*>, <Class *http://vocab.gtfs.org/terms#DropOffType*>, <Class *http://vocab.gtfs.org/terms#FareClass*>, <Class *http://vocab.gtfs.org/terms#FareRule*>, <Class *http://vocab.gtfs.org/terms#Feed*>, <Class *http://vocab.gtfs.org/terms#Frequency*>, <Class *http://vocab.gtfs.org/terms#PaymentMethod*>, <Class *http://vocab.gtfs.org/terms#PickupType*>, <Class *http://vocab.gtfs.org/terms#Route*>, <Class *http://vocab.gtfs.org/terms#RouteType*>, <Class *http://vocab.gtfs.org/terms#Service*>, <Class *http://vocab.gtfs.org/terms#ServiceRule*>, <Class *http://vocab.gtfs.org/terms#Shape*>, <Class *http://vocab.gtfs.org/terms#ShapePoint*>, <Class *http://vocab.gtfs.org/terms#Station*>, <Class *http://vocab.gtfs.org/terms#Stop*>, <Class *http://vocab.gtfs.org/terms#StopTime*>, <Class *http://vocab.gtfs.org/terms#Transfer*>, <Class *http://vocab.gtfs.org/terms#TransferRule*>, <Class *http://vocab.gtfs.org/terms#TransferType*>, <Class *http://vocab.gtfs.org/terms#TransfersAllowedType*>, <Class *http://vocab.gtfs.org/terms#Trip*>, <Class *http://vocab.gtfs.org/terms#WheelchairBoardingStatus*>, <Class *http://vocab.gtfs.org/terms#Zone*>, <Class *http://schema.org/PriceSpecification*>]\n"
     ]
    }
   ],
   "source": [
    "connection, cursor = connect_database()\n",
    "tables = get_tables(connection,cursor)\n",
    "\n",
    "for y in tables:\n",
    "        sql_select_Query = \"select * from \" + y[0]\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(sql_select_Query)\n",
    "        # get all records\n",
    "        records = cursor.fetchall()\n",
    "        print(\"Total number of columns in table: \" + y[0], cursor.column_names)\n",
    "\n",
    "#'http://vocab.gtfs.org/terms#' 'http://xmlns.com/foaf/0.1/'\n",
    "ontology = loadOntology('http://vocab.gtfs.org/terms#')\n",
    "print(\"\\nReading  Ontology...\\n\")\n",
    "oc_classes = get_onto_classes(ontology)\n",
    "print(\"By library classes method: \", oc_classes)\n",
    "oc_properties = get_onto_properties(ontology, oc_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMLEntity(object):\n",
    "    def __init__(self, table, onto_class, class_properties):\n",
    "        self.onto_class = onto_class\n",
    "        self.onto_properties = class_properties\n",
    "        self.table = table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[<Property *http://vocab.gtfs.org/terms#agency*>, 'agency_id'], [<Property *http://vocab.gtfs.org/terms#fareUrl*>, 'agency_url']]\n",
      "<Class *http://vocab.gtfs.org/terms#Agency*>\n",
      "[]\n",
      "<Class *http://vocab.gtfs.org/terms#CalendarRule*>\n",
      "[[<Property *http://vocab.gtfs.org/terms#dateAddition*>, 'date']]\n",
      "<Class *http://vocab.gtfs.org/terms#CalendarDateRule*>\n",
      "[]\n",
      "<Class *http://vocab.gtfs.org/terms#Feed*>\n",
      "[[<Property *http://vocab.gtfs.org/terms#endTime*>, 'end_time'], [<Property *http://vocab.gtfs.org/terms#headwaySeconds*>, 'headway_secs'], [<Property *http://vocab.gtfs.org/terms#startTime*>, 'start_time'], [<Property *http://vocab.gtfs.org/terms#usesExactTimes*>, 'exact_times']]\n",
      "<Class *http://vocab.gtfs.org/terms#Frequency*>\n",
      "[[<Property *http://vocab.gtfs.org/terms#longName*>, 'route_long_name'], [<Property *http://vocab.gtfs.org/terms#route*>, 'route_id'], [<Property *http://vocab.gtfs.org/terms#routeType*>, 'route_type'], [<Property *http://vocab.gtfs.org/terms#shortName*>, 'route_short_name']]\n",
      "<Class *http://vocab.gtfs.org/terms#Route*>\n",
      "[[<Property *http://vocab.gtfs.org/terms#distanceTraveled*>, 'shape_dist_traveled'], [<Property *http://vocab.gtfs.org/terms#shape*>, 'shape_id'], [<Property *http://vocab.gtfs.org/terms#shapePoint*>, 'shape_id']]\n",
      "<Class *http://vocab.gtfs.org/terms#Shape*>\n",
      "[[<Property *http://vocab.gtfs.org/terms#destinationStop*>, 'location_type'], [<Property *http://vocab.gtfs.org/terms#originStop*>, 'location_type'], [<Property *http://vocab.gtfs.org/terms#parentStation*>, 'parent_station'], [<Property *http://vocab.gtfs.org/terms#stop*>, 'stop_id'], [<Property *http://vocab.gtfs.org/terms#wheelchairAccessible*>, 'wheelchair_boarding']]\n",
      "<Class *http://vocab.gtfs.org/terms#Stop*>\n",
      "[[<Property *http://vocab.gtfs.org/terms#arrivalTime*>, 'arrival_time'], [<Property *http://vocab.gtfs.org/terms#departureTime*>, 'departure_time'], [<Property *http://vocab.gtfs.org/terms#distanceTraveled*>, 'shape_dist_traveled'], [<Property *http://vocab.gtfs.org/terms#stopSequence*>, 'stop_sequence']]\n",
      "<Class *http://vocab.gtfs.org/terms#StopTime*>\n",
      "[[<Property *http://vocab.gtfs.org/terms#block*>, 'block_id'], [<Property *http://vocab.gtfs.org/terms#route*>, 'route_id'], [<Property *http://vocab.gtfs.org/terms#shortName*>, 'trip_short_name'], [<Property *http://vocab.gtfs.org/terms#trip*>, 'trip_id']]\n",
      "<Class *http://vocab.gtfs.org/terms#Trip*>\n"
     ]
    }
   ],
   "source": [
    "def levenshteinDistanceDP(token1, token2):\n",
    "    distances = numpy.zeros((len(token1) + 1, len(token2) + 1))\n",
    "\n",
    "    for t1 in range(len(token1) + 1):\n",
    "        distances[t1][0] = t1\n",
    "\n",
    "    for t2 in range(len(token2) + 1):\n",
    "        distances[0][t2] = t2\n",
    "        \n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    \n",
    "    for t1 in range(1, len(token1) + 1):\n",
    "        for t2 in range(1, len(token2) + 1):\n",
    "            if (token1[t1-1] == token2[t2-1]):\n",
    "                distances[t1][t2] = distances[t1 - 1][t2 - 1]\n",
    "            else:\n",
    "                a = distances[t1][t2 - 1]\n",
    "                b = distances[t1 - 1][t2]\n",
    "                c = distances[t1 - 1][t2 - 1]\n",
    "                \n",
    "                if (a <= b and a <= c):\n",
    "                    distances[t1][t2] = a + 1\n",
    "                elif (b <= a and b <= c):\n",
    "                    distances[t1][t2] = b + 1\n",
    "                else:\n",
    "                    distances[t1][t2] = c + 1\n",
    "    return distances[len(token1)][len(token2)]\n",
    "    \n",
    "#min(1, 1 - abs(t1-t2)/len(token1))\n",
    "\n",
    "def string_distance(str1, str2):\n",
    "    distances = numpy.zeros(len(str1))\n",
    "    streak = 0\n",
    "    for x in range(0, len(str1)):\n",
    "        for y in range(x, len(str2)):\n",
    "            char1 = str1[x].lower()\n",
    "            char2 = str2[y].lower()\n",
    "            if char1 == char2:\n",
    "                val = min(1, 1 - abs(x-y)/(max(len(str1), len(str2))))\n",
    "                #print(val)\n",
    "                if distances[x] < val:\n",
    "                    distances[x] = val + streak\n",
    "                if val == 1:\n",
    "                    streak += 0.1\n",
    "                    break\n",
    "    #print(distances)\n",
    "    return numpy.sum(distances)/(max(len(str1), len(str2)))\n",
    "\n",
    "def correspondenceClass(oc_classes, table):\n",
    "    dis = 100\n",
    "    term = \"\"\n",
    "    for x in oc_classes:\n",
    "        #print(x[0])\n",
    "        term1 = str(table).replace('_', '').lower()\n",
    "        term2 = x.qname.split(':').pop()\n",
    "        #print(\"Term1: \", term1)\n",
    "        #print(\"Term2: \", term2)\n",
    "        auxDis = levenshteinDistanceDP(term1, term2)\n",
    "        #print(auxDis)\n",
    "        if auxDis < dis:\n",
    "            dis = auxDis\n",
    "            #properties = get_class_properties(ontology, x)\n",
    "            #propertiesCorrespondence = getPropertiesCorrespondence(columns, properties)\n",
    "            term = x\n",
    "    return term\n",
    "\n",
    "def getPropertiesCorrespondence(columns, properties):\n",
    "    cor_properties = []\n",
    "    for x in properties:\n",
    "        dis = 100\n",
    "        term = []\n",
    "        for y in columns:\n",
    "            term1 = str(y).replace('_', '').lower()\n",
    "            term2 = x.qname.split(':').pop()\n",
    "            auxDis = levenshteinDistanceDP(term1, term2)\n",
    "            if auxDis < dis:\n",
    "                dis = auxDis\n",
    "                term = [x, y]\n",
    "        cor_properties.append(term)\n",
    "    return cor_properties\n",
    "\n",
    "def getRMLEntities(tables, oc_classes, ontology, cursor):\n",
    "    classes = []\n",
    "    for x in range(0, len(tables)):\n",
    "        columns = get_table_properties(tables[x][0], cursor)\n",
    "        #classes.append([tables[x], correspondenceClass(oc_classes, tables[x], ontology, columns)])\n",
    "        onto_class = correspondenceClass(oc_classes, tables[x])\n",
    "        properties = get_class_properties(ontology, onto_class)\n",
    "        onto_properties = getPropertiesCorrespondence(columns, properties)\n",
    "        classes.append(RMLEntity(tables[x], onto_class, onto_properties))\n",
    "    return classes\n",
    "\n",
    "\n",
    "\n",
    "classCor = getRMLEntities(tables, oc_classes, ontology, cursor)\n",
    "for x in classCor:\n",
    "    print(x.onto_properties)\n",
    "    print(x.onto_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL Server version  8.0.26\n",
      "You're connected to database:  ('gtfs',)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'my_world' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37094/4020585776.py\u001b[0m in \u001b[0;36mget_ontology_classes\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         l = list(my_world.sparql(\"\"\"\n\u001b[0m\u001b[1;32m      4\u001b[0m                    \u001b[0mSELECT\u001b[0m  \u001b[0mDISTINCT\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m?\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_world' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37094/1694391880.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mtables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0monto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadOntology\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gtfs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0montologyClasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ontology_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mRML_Transformation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'properties.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0montologyClasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_37094/4020585776.py\u001b[0m in \u001b[0;36mget_ontology_classes\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m             \"\"\"))\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         l = list(my_world.sparql(\"\"\"\n\u001b[0m\u001b[1;32m      9\u001b[0m                    \u001b[0mSELECT\u001b[0m  \u001b[0mDISTINCT\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m?\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                    \u001b[0;34m{\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m?\u001b[0m\u001b[0mx\u001b[0m \u001b[0ma\u001b[0m \u001b[0mowl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mClass\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_world' is not defined"
     ]
    }
   ],
   "source": [
    "def loadProperties(outputFile, propertiesFile):\n",
    "    with open('properties/' + propertiesFile) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        json_file.close()\n",
    "    return data\n",
    "\n",
    "def writeProperties(output, properties):\n",
    "    with open('outputs/' + output, 'w') as fw:\n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = fw\n",
    "        for x in properties['list_prefixes']:\n",
    "            print('@prefix ' + x['prefix'] + \": \" + x['URI'])\n",
    "        print('@base ' + properties['base']['URI'])\n",
    "        sys.stdout = original_stdout\n",
    "        fw.close()\n",
    "#print(data['list_prefixes'])\n",
    "\n",
    "def LogicalSource(table, SQLversion):\n",
    "    level = 8\n",
    "    print (\" \"*level, \"rml:logicalSource [\")\n",
    "    print (\" \"*(2*level), \"rml:source: \" + '\"' + \"<#DB_source>\" + '\"')\n",
    "    print (\" \"*(2*level), \"rr:sqlVersion \" + '\"' + SQLversion + '\"')\n",
    "    print (\" \"*(2*level), \"rr:tableName: \" + '\"' + table[0] + '\"')\n",
    "    print (\" \"*level, \"];\")\n",
    "    \n",
    "def TripleMap(subject):\n",
    "    level = 8\n",
    "    print (\"<\" + subject[0] + \">\")\n",
    "    print (\" \"*level, \"a rr:TriplesMap;\")\n",
    "    #print (\"\\n\")\n",
    "    LogicalSource(subject, \"SQL2008\")\n",
    "    \n",
    "#def SubjectMap(ontologyClasses):\n",
    "    \n",
    "\n",
    "def deleteFileContent(file):\n",
    "    file = open(\"outputs/\" + file,\"r+\")\n",
    "    file.truncate(0)\n",
    "    file.close()\n",
    "#def RMLTable(oc, table):\n",
    "\n",
    "def RML_Transformation(output, properties, tables, ontologyClasses):\n",
    "    data = loadProperties(output, properties)\n",
    "    writeProperties(output, data)\n",
    "    with open('outputs/' + output, 'w') as fw:\n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = fw\n",
    "        for x in tables:\n",
    "            TripleMap(x)\n",
    "        sys.stdout = original_stdout\n",
    "        fw.close()\n",
    "        \n",
    "connection, cursor = connect_database()\n",
    "tables = get_tables(connection,cursor)\n",
    "onto = loadOntology(\"gtfs\")\n",
    "ontologyClasses = get_ontology_classes()\n",
    "RML_Transformation('output.txt', 'properties.json', tables, ontologyClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
